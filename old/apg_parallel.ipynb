{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=12'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "from brax.training import normalization\n",
    "\n",
    "\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.envs import create_fn\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from controllers import GruController, MlpController, LinearController\n",
    "from common import do_local_apg, add_guassian_noise, add_uniform_noise, add_uniform_and_pareto_noise, add_sym_pareto_noise, do_one_rollout\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "def visualize(sys, qps, height=480):\n",
    "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
    "  return HTML(html.render(sys, qps, height=height))\n",
    "\n",
    "len(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length = 1000\n",
    "action_repeat = 1\n",
    "batch_size = jax.local_device_count()\n",
    "#noise_std = 0.2\n",
    "\n",
    "noise_scale = 2.0\n",
    "noise_beta = 2.0\n",
    "\n",
    "\n",
    "apg_epochs = 500\n",
    "batch_size = 1\n",
    "truncation_length = 10\n",
    "learning_rate = 1e-3\n",
    "clipping = 1e9\n",
    "\n",
    "normalize_observations=False\n",
    "\n",
    "env_name = \"inverted_double_pendulum\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=None, auto_reset=False)\n",
    "env = env_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "reset_keys = jax.random.split(key, num=jax.local_device_count())\n",
    "model_keys = jax.random.split(reset_keys[0], num=jax.local_device_count())\n",
    "noise_keys = jax.random.split(model_keys[0], num=jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearController(env.observation_size,env.action_size)\n",
    "normalizer_params, obs_normalizer_update_fn, obs_normalizer_apply_fn = normalization.create_observation_normalizer(\n",
    "          env.observation_size, normalize_observations, num_leading_batch_dims=1)\n",
    "\n",
    "do_apg_pmap = jax.pmap(do_local_apg, in_axes = (None,None,None,None,0,0,None,None,None,None,None,None), static_broadcasted_argnums=(0,1,2,6,7,8,9,10,11,12))\n",
    "do_rollout_pmap = jax.pmap(do_one_rollout, in_axes = (None,None,None,0,0,None,None,None), static_broadcasted_argnums=(0,1,5,6,7))\n",
    "\n",
    "\n",
    "init_states = jax.pmap(env.reset)(reset_keys)\n",
    "x0 = init_states.obs\n",
    "h0 = jnp.zeros(env.observation_size)\n",
    "\n",
    "policy_params = jax.vmap(policy.init, in_axes=(0,None,None))(model_keys, h0, x0)\n",
    "\n",
    "best_reward = -float('inf')\n",
    "meta_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "noise_keys = jax.random.split(noise_keys[0], num=jax.local_device_count())\n",
    "train_keys = jax.random.split(noise_keys[0], num=jax.local_device_count())\n",
    "\n",
    "policy_params = policy_params.unfreeze()\n",
    "policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[0].set(1649.0)\n",
    "policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[1].set(460.2)\n",
    "policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[2].set(716.1)\n",
    "policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[3].set(278.2)\n",
    "policy_params = flax.core.frozen_dict.FrozenDict(policy_params)\n",
    "\n",
    "rewards_before, obs, acts, states_before = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "normalizer_params = obs_normalizer_update_fn(normalizer_params, obs[0,:])\n",
    "policy_params_trained, rewards_lists = do_apg_pmap(apg_epochs, env_fn, policy.apply, normalizer_params, policy_params, train_keys, learning_rate, episode_length, action_repeat, normalize_observations, batch_size, clipping, truncation_length)\n",
    "rewards_after, obs, acts, states_after = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "\n",
    "print(jnp.any(policy_params_trained['params']['Dense_0']['kernel'] - policy_params['params']['Dense_0']['kernel']))\n",
    "\n",
    "top_idx = sorted(range(len(rewards_lists)), key=lambda k: jnp.mean(rewards_lists[k][-5:]), reverse=True)\n",
    "\n",
    "_, params_def = jax.tree_flatten(policy_params)\n",
    "params_flat, _ = jax.tree_flatten(policy_params_trained)\n",
    "top_params_flat = [param[top_idx[0]] for param in params_flat]\n",
    "top_params = jax.tree_unflatten(params_def, top_params_flat)\n",
    "\n",
    "#     _, norm_def = jax.tree_flatten(normalizer_params)\n",
    "#     norm_flat, _ = jax.tree_flatten(normalizer_params_all)\n",
    "#     top_norm_flat = [param[top_idx[0]] for param in norm_flat]\n",
    "#     top_norms = jax.tree_unflatten(norm_def, top_norm_flat)\n",
    "\n",
    "\n",
    "meta_rewards_list.append(best_reward)\n",
    "\n",
    "print(f\" Iteration 1 --------------------------------\")\n",
    "\n",
    "for j in range(len(top_idx)):\n",
    "    done_idx = jnp.where(states_before.done[top_idx[j], :], size=1)[0].item()\n",
    "    if done_idx == 0:\n",
    "        done_idx = rewards_before.shape[-1]\n",
    "    rewards_sum_before = jnp.sum(rewards_before[top_idx[j],:done_idx])\n",
    "\n",
    "    done_idx = jnp.where(states_after.done[top_idx[j], :], size=1)[0].item()\n",
    "    if done_idx == 0:\n",
    "        done_idx = rewards_after.shape[-1]\n",
    "    rewards_sum_after = jnp.sum(rewards_after[top_idx[j],:done_idx])\n",
    "\n",
    "    print(f\"{j} : reward: {rewards_sum_before} -> {jnp.mean(rewards_lists[top_idx[j]][-5:])}  |  {rewards_sum_after}\")\n",
    "\n",
    "\n",
    "plt.plot(rewards_lists[top_idx[0]])\n",
    "plt.figure()\n",
    "\n",
    "#print(f\"{i} : best reward: {rewards_sum_before} -> {rewards_lists[top_idx[0]][-1]}  |  {rewards_sum_after}\")\n",
    "\n",
    "print(\"Best reward so far: \", best_reward)\n",
    "print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brax.jumpy as jp\n",
    "@jax.jit\n",
    "def do_rnn_rollout(policy_params, normalizer_params, key):\n",
    "    init_state = env.reset(key)\n",
    "    h0 = jp.zeros_like(init_state.obs)\n",
    "\n",
    "    def do_one_rnn_step(carry, step_idx):\n",
    "        state, h, policy_params, normalizer_params  = carry\n",
    "\n",
    "        normed_obs = obs_normalizer_apply_fn(normalizer_params, state.obs)\n",
    "        h1 , actions = policy.apply(policy_params, h, normed_obs)\n",
    "        #actions = jp.ones_like(actions)*0.0\n",
    "        nstate = env.step(state, actions)    \n",
    "        #h1 = jax.lax.cond(nstate.done, lambda x: jnp.zeros_like(h1), lambda x: h1, None)\n",
    "        return (jax.lax.stop_gradient(nstate), h1, policy_params, normalizer_params), (nstate.reward,state.obs, actions, nstate)\n",
    "\n",
    "\n",
    "    _, (rewards, obs, acts, states) = jp.scan(\n",
    "        do_one_rnn_step, (init_state, h0, policy_params, normalizer_params),\n",
    "        (jnp.array(range(episode_length // action_repeat))),\n",
    "        length=episode_length // action_repeat)\n",
    "\n",
    "    return rewards, obs, acts, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, reset_key = jax.random.split(key)\n",
    "(rewards, obs, acts, states) = do_rnn_rollout(top_params, normalizer_params, reset_key)\n",
    "\n",
    "done_idx = jnp.where(states.done, size=1)[0].item()\n",
    "if done_idx == 0:\n",
    "    done_idx = states.done.shape[0]\n",
    "rewards_sum = jnp.sum(rewards[:done_idx])\n",
    "\n",
    "plt.plot(obs);\n",
    "plt.figure()\n",
    "plt.plot(acts);\n",
    "print(rewards_sum)\n",
    "print(states.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, obs, acts, states = do_one_rollout(env_fn, policy.apply, normalizer_params, top_params, key, episode_length, action_repeat, normalize_observations)\n",
    "print(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_flat, qp_def = jax.tree_flatten(states.qp)\n",
    "\n",
    "qp_list = []\n",
    "\n",
    "for i in range(qp_flat[0].shape[0]):\n",
    "    qpc=[]\n",
    "    for thing in qp_flat:\n",
    "        qpc.append(thing[i,:])\n",
    "    qp_list.append(jax.tree_unflatten(qp_def, qpc))\n",
    "    \n",
    "\n",
    "visualize(env.sys, qp_list, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_params_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(obs**2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brax",
   "language": "python",
   "name": "brax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
