{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=24'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "from brax.training import normalization\n",
    "\n",
    "\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.envs import create_fn\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from controllers import GruController, MlpController, LinearController\n",
    "from common import do_local_apg, add_guassian_noise, add_guassian_noise_mixed_std, do_one_rollout\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "def visualize(sys, qps, height=480):\n",
    "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
    "  return HTML(html.render(sys, qps, height=height))\n",
    "\n",
    "len(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length = 500\n",
    "action_repeat = 1\n",
    "apg_epochs = 75\n",
    "ars_epochs = 50\n",
    "batch_size = 1\n",
    "truncation_length = None\n",
    "learning_rate = 3e-4\n",
    "clipping = 1e9\n",
    "\n",
    "initial_std = 0.03\n",
    "step_size = 0.02\n",
    "num_elite = 8\n",
    "eps = 1e-6\n",
    "\n",
    "normalize_observations=True\n",
    "\n",
    "env_name = \"inverted_pendulum_swingup\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=None, auto_reset=False)\n",
    "env = env_fn()\n",
    "\n",
    "#policy = GruController(env.observation_size, env.action_size, 16)\n",
    "policy = LinearController(env.observation_size,env.action_size)\n",
    "#policy = GruController(env.observation_size, env.action_size, 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_directions = jax.local_device_count()\n",
    "key = jax.random.PRNGKey(0)\n",
    "reset_keys = jax.random.split(key, num=num_directions)\n",
    "noise_keys = jax.random.split(reset_keys[0], num=num_directions)\n",
    "_, model_key = jax.random.split(noise_keys[0])\n",
    "\n",
    "\n",
    "normalizer_params, obs_normalizer_update_fn, obs_normalizer_apply_fn = normalization.create_observation_normalizer(\n",
    "          env.observation_size, normalize_observations, num_leading_batch_dims=1)\n",
    "\n",
    "add_noise_pmap = jax.pmap(add_guassian_noise, in_axes=(None,None,0))\n",
    "do_apg_pmap = jax.pmap(do_local_apg, in_axes = (None,None,None,None,0,0,None,None,None,None,None,None), static_broadcasted_argnums=(0,1,2,6,7,8,9,10,11,12))\n",
    "do_rollout_pmap = jax.pmap(do_one_rollout, in_axes = (None,None,None,0,0,None,None,None), static_broadcasted_argnums=(0,1,5,6,7))\n",
    "\n",
    "init_states = jax.pmap(env.reset)(reset_keys)\n",
    "x0 = init_states.obs\n",
    "h0 = jnp.zeros(env.observation_size)\n",
    "\n",
    "policy_params = policy.init(model_key, h0, x0)\n",
    "\n",
    "new_params_flat = []\n",
    "policy_params_flat, policy_params_def = jax.tree_flatten(policy_params)\n",
    "\n",
    "for p in policy_params_flat:\n",
    "    new_params_flat.append(jnp.zeros_like(p))\n",
    "\n",
    "policy_params = jax.tree_unflatten(policy_params_def, new_params_flat)   \n",
    "\n",
    "best_reward = -float('inf')\n",
    "meta_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before apg -4929.81\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'policy_params_with_noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30349/3017648859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbase_reward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"before apg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_reward_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpolicy_params_with_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_apg_pmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapg_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_params_with_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_repeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mreward_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrew\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrewards_lists\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtop_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreward_sums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_params_with_noise' is not defined"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import time\n",
    "\n",
    "policy_params_flat, policy_params_def = jax.tree_flatten(policy_params)\n",
    "noise_std = initial_std\n",
    "best_reward_list = []\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    noise_keys = jax.random.split(noise_keys[0], num=num_directions)\n",
    "    train_keys = jax.random.split(noise_keys[0], num=num_directions)\n",
    "    \n",
    "    base_rewards, _, _, _ = do_one_rollout(env_fn, policy.apply, normalizer_params, policy_params, train_keys[0], episode_length, action_repeat, normalize_observations)\n",
    "    base_reward_sum = sum(base_rewards)\n",
    "    print(\"before apg\", base_reward_sum)\n",
    "    policy_params_with_noise, rewards_lists = do_apg_pmap(apg_epochs, env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, learning_rate, episode_length, action_repeat, normalize_observations, batch_size, clipping, truncation_length)\n",
    "    reward_sums = [jnp.mean(rew[-5:]) for rew in rewards_lists]\n",
    "    top_idx = sorted(range(len(reward_sums)), key=lambda k: reward_sums[k], reverse=True)\n",
    "\n",
    "    print(\"after apg, before ars: \", reward_sums[top_idx[0]])\n",
    "\n",
    "    \n",
    "    for j in range(ars_epochs):\n",
    "\n",
    "\n",
    "        params_with_noise_flat, params_with_noise_def = jax.tree_flatten(policy_params_with_noise)\n",
    "        noise_flat, noise_def = jax.tree_flatten(noise)\n",
    "        \n",
    "        params_elite_flat = [p[top_idx[:num_elite], :] for p in params_with_noise_flat]\n",
    "        noise_elite_flat = [p[top_idx[:num_elite], :] for p in noise_flat]\n",
    "        \n",
    "        reward_sums_elite = jnp.array([reward_sums[t] for t in top_idx][:num_elite])\n",
    "        \n",
    "        reward_std = jnp.std(reward_sums_elite)\n",
    "            \n",
    "        new_params_flat = []\n",
    "        for k, (old_params, elite_noise) in enumerate(zip(policy_params_flat, noise_elite_flat)):\n",
    "            reward_weight = reward_sums_elite - base_reward_sum\n",
    "            reward_weight = reward_weight.reshape((num_elite, *[1 for _ in range(len(elite_noise.shape)-1)]))\n",
    "\n",
    "            new_params_flat.append(old_params + step_size/(num_elite*reward_std) * jnp.sum(reward_weight*elite_noise, axis=0))\n",
    "        \n",
    "        policy_params = jax.tree_unflatten(policy_params_def, new_params_flat)   \n",
    "        policy_params_flat, policy_params_def = jax.tree_flatten(policy_params)\n",
    "        \n",
    "        base_rewards, _, _, _ = do_one_rollout(env_fn, policy.apply, normalizer_params, policy_params, train_keys[0], episode_length, action_repeat, normalize_observations)\n",
    "        base_reward_sum = sum(base_rewards)\n",
    "        \n",
    "        policy_params_with_noise, policy_params_with_anti_noise, noise = add_noise_pmap(policy_params, noise_std, noise_keys)\n",
    "        rewards, obs, acts, states_before = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "        reward_sums = jnp.sum(rewards, axis=1)\n",
    "        top_idx = sorted(range(len(reward_sums)), key=lambda k: reward_sums[k], reverse=True)\n",
    "\n",
    "        \n",
    "    print(\"after ars\", reward_sums[top_idx[0]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "import time\n",
    "\n",
    "policy_params_flat, policy_params_def = jax.tree_flatten(policy_params)\n",
    "noise_std = initial_std\n",
    "best_reward_list = []\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    noise_keys = jax.random.split(noise_keys[0], num=num_directions)\n",
    "    train_keys = jax.random.split(noise_keys[0], num=num_directions)\n",
    "\n",
    "    base_rewards, _, _, _ = do_one_rollout(env_fn, policy.apply, normalizer_params, policy_params, train_keys[0], episode_length, action_repeat, normalize_observations)\n",
    "    base_reward_sum = sum(base_rewards)\n",
    "    policy_params_with_noise, policy_params_with_anti_noise, noise = add_noise_pmap(policy_params, noise_std, noise_keys)\n",
    "    \n",
    "    rewards_before, obs, acts, states_before = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "    normalizer_params = obs_normalizer_update_fn(normalizer_params, obs[0,:])\n",
    "\n",
    "    #policy_params_with_noise, rewards_lists = do_apg_pmap(apg_epochs, env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, learning_rate, episode_length, action_repeat, normalize_observations, batch_size, clipping, truncation_length)\n",
    "    reward_sums = [rew[-1] for rew in rewards_lists]\n",
    "    top_idx = sorted(range(len(reward_sums)), key=lambda k: reward_sums[k], reverse=True)\n",
    "    \n",
    "    best_reward_list.append(jnp.mean(rewards_lists[top_idx[0]][-5:]))    \n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print(f\" Iteration {i} --------------------------------\")\n",
    "        #print(f\" Time: {time.time() - start}\")\n",
    "\n",
    "        for j in range(len(top_idx)):\n",
    "            done_idx = jnp.where(states_before.done[top_idx[j], :], size=1)[0].item()\n",
    "            if done_idx == 0:\n",
    "                done_idx = rewards_before.shape[-1]\n",
    "            rewards_sum_before = jnp.sum(rewards_before[top_idx[j],:done_idx])\n",
    "\n",
    "            print(f\"{j} : reward: {rewards_sum_before} -> {reward_sums[top_idx[j]]}\")\n",
    "            if j == num_elite-1:\n",
    "                print(\"---\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print()\n",
    "    \n",
    "    for j in range(ars_epochs):\n",
    "        top_idx = sorted(range(len(reward_sums)), key=lambda k: reward_sums[k], reverse=True)\n",
    "\n",
    "        params_with_noise_flat, params_with_noise_def = jax.tree_flatten(policy_params_with_noise)\n",
    "        noise_flat, noise_def = jax.tree_flatten(noise)\n",
    "        \n",
    "        params_elite_flat = [p[top_idx[:num_elite], :] for p in params_with_noise_flat]\n",
    "        noise_elite_flat = [p[top_idx[:num_elite], :] for p in noise_flat]\n",
    "        reward_sums_elite = jnp.array([reward_sums[t] for t in top_idx][:num_elite])\n",
    "        reward_std = jnp.std(reward_sums_elite)\n",
    "            \n",
    "        new_params_flat = []\n",
    "        for k, (old_params, elite_noise) in enumerate(zip(policy_params_flat, noise_elite_flat)):\n",
    "            reward_weight = reward_sums_elite - base_reward_sum\n",
    "            reward_weight = reward_weight.reshape((num_elite, *[1 for _ in range(len(elite_noise.shape)-1)]))\n",
    "\n",
    "            new_params_flat.append(old_params + step_size/(n_elite*reward_std) * jnp.sum(reward_weight*elite_noise, axis=0))\n",
    "        \n",
    "        policy_params = jax.tree_unflatten(policy_params_def, new_params_flat)   \n",
    "        policy_params_flat, policy_params_def = jax.tree_flatten(policy_params)\n",
    "\n",
    "        policy_params_with_noise, _,  noise = add_noise_pmap(policy_params, noise_std, noise_keys)\n",
    "        rewards, obs, acts, states_before = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "        reward_sums = jnp.sum(rewards, axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brax.jumpy as jp\n",
    "@jax.jit\n",
    "def do_rnn_rollout(policy_params, normalizer_params, key):\n",
    "    init_state = env.reset(key)\n",
    "    h0 = jp.zeros_like(init_state.obs)\n",
    "\n",
    "    def do_one_rnn_step(carry, step_idx):\n",
    "        state, h, policy_params, normalizer_params  = carry\n",
    "\n",
    "        normed_obs = obs_normalizer_apply_fn(normalizer_params, state.obs)\n",
    "        h1 , actions = policy.apply(policy_params, h, normed_obs)\n",
    "        #actions = jp.ones_like(actions)*0.0\n",
    "        nstate = env.step(state, actions)    \n",
    "        #h1 = jax.lax.cond(nstate.done, lambda x: jnp.zeros_like(h1), lambda x: h1, None)\n",
    "        return (jax.lax.stop_gradient(nstate), h1, policy_params, normalizer_params), (nstate.reward,state.obs, actions, nstate)\n",
    "\n",
    "\n",
    "    _, (rewards, obs, acts, states) = jp.scan(\n",
    "        do_one_rnn_step, (init_state, h0, policy_params, normalizer_params),\n",
    "        (jnp.array(range(episode_length // action_repeat))),\n",
    "        length=episode_length // action_repeat)\n",
    "\n",
    "    return rewards, obs, acts, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, reset_key = jax.random.split(key)\n",
    "(rewards, obs, acts, states) = do_rnn_rollout(policy_params, normalizer_params, reset_key)\n",
    "\n",
    "done_idx = jnp.where(states.done, size=1)[0].item()\n",
    "if done_idx == 0:\n",
    "    done_idx = states.done.shape[0]\n",
    "rewards_sum = jnp.sum(rewards[:done_idx])\n",
    "\n",
    "plt.plot(obs[:,0]);\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(acts);\n",
    "print(rewards_sum)\n",
    "print(states.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_flat, qp_def = jax.tree_flatten(states.qp)\n",
    "\n",
    "qp_list = []\n",
    "\n",
    "for i in range(qp_flat[0].shape[0]):\n",
    "    qpc=[]\n",
    "    for thing in qp_flat:\n",
    "        qpc.append(thing[i,:])\n",
    "    qp_list.append(jax.tree_unflatten(qp_def, qpc))\n",
    "    \n",
    "\n",
    "visualize(env.sys, qp_list, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brax",
   "language": "python",
   "name": "brax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
