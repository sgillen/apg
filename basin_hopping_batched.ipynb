{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "from brax.training import normalization\n",
    "\n",
    "\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.envs import create_fn\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from controllers import GruController, MlpController\n",
    "from common import do_local_apg, add_guassian_noise, add_uniform_noise, add_uniform_and_pareto_noise, add_sym_pareto_noise, do_one_rollout\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "jax.config.update('jax_platform_name', 'gpu')\n",
    "\n",
    "def visualize(sys, qps, height=480):\n",
    "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
    "  return HTML(html.render(sys, qps, height=height))\n",
    "\n",
    "len(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length = 500\n",
    "action_repeat = 1\n",
    "batch_size = jax.local_device_count()\n",
    "#noise_std = 0.2\n",
    "\n",
    "noise_scale = 5.0\n",
    "noise_beta = 1.8\n",
    "\n",
    "\n",
    "apg_epochs = 50\n",
    "batch_size = 16\n",
    "truncation_length = 50\n",
    "learning_rate = 1e-4\n",
    "clipping = 1e9\n",
    "\n",
    "normalize_observations=True\n",
    "\n",
    "env_name = \"reacher\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=batch_size, auto_reset=False)\n",
    "env = env_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key, reset_key = jax.random.split(key)\n",
    "key, model_key = jax.random.split(key)\n",
    "key, noise_key = jax.random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GruController(env.observation_size,env.action_size,64)\n",
    "normalizer_params, obs_normalizer_update_fn, obs_normalizer_apply_fn = normalization.create_observation_normalizer(\n",
    "          env.observation_size, normalize_observations, num_leading_batch_dims=1)\n",
    "\n",
    "init_states = env.reset(reset_key)\n",
    "x0 = init_states.obs\n",
    "h0 = jnp.zeros(env.observation_size)\n",
    "\n",
    "policy_params = policy.init(model_key, h0, x0)\n",
    "\n",
    "best_reward = -float('inf')\n",
    "meta_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "true_fun and false_fun output must have identical types, got\n((State(qp=QP(pos='ShapedArray(float32[16,4,3])', rot='ShapedArray(float32[16,4,4])', vel='ShapedArray(float32[16,4,3])', ang='ShapedArray(float32[16,4,3])'), obs='ShapedArray(float32[16,11])', reward='ShapedArray(float32[16])', done='ShapedArray(float32[16])', metrics={'rewardCtrl': 'ShapedArray(float32[16])', 'rewardDist': 'ShapedArray(float32[16])'}, info={'steps': 'ShapedArray(float32[16])', 'truncation': 'ShapedArray(float32[16])'}), 'ShapedArray(float32[16,11])', FrozenDict({\n    params: {\n        Dense_0: {\n            bias: 'ShapedArray(float32[64])',\n            kernel: 'ShapedArray(float32[11,64])',\n        },\n        Dense_1: {\n            bias: 'ShapedArray(float32[64])',\n            kernel: 'ShapedArray(float32[64,64])',\n        },\n        Dense_2: {\n            bias: 'ShapedArray(float32[2])',\n            kernel: 'ShapedArray(float32[64,2])',\n        },\n        GRUCell_0: {\n            hn: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            hr: {\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            hz: {\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            in: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            ir: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            iz: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n        },\n    },\n}), ('ShapedArray(float32[])', 'ShapedArray(float32[11])', 'ShapedArray(float32[11])'), 'DIFFERENT ShapedArray(float32[], weak_type=True) vs. ShapedArray(float32[16])', 'ShapedArray(int32[], weak_type=True)'), None).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "\u001b[0;32m~/work/apg/common.py\u001b[0m in \u001b[0;36mdo_local_apg\u001b[0;34m(num_epochs, env_fn, policy_apply, normalizer_params, policy_params, key, learning_rate, episode_length, action_repeat, normalize_observations, batch_size, clipping, truncation_length)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_params2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_one_apg_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_params2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[0;32m~/work/apg/common.py\u001b[0m in \u001b[0;36mdo_one_apg_iter\u001b[0;34m(carry, epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreset_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mparam_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mparam_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "\u001b[0;32m~/work/apg/common.py\u001b[0m in \u001b[0;36msum_rnn_loss\u001b[0;34m(policy_params, normalizer_params, key)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msum_rnn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mrewards_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap_rnn_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_sums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[0;32m~/work/apg/common.py\u001b[0m in \u001b[0;36mdo_training_rollout\u001b[0;34m(policy_params, normalizer_params, key)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_one_rnn_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         carry , _  = jax.lax.scan(\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mbody_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[0;32m~/work/apg/common.py\u001b[0m in \u001b[0;36mbody_fn\u001b[0;34m(carry, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbody_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_one_rnn_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         carry , _  = jax.lax.scan(\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/brax/lib/python3.9/site-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_check_tree_and_avals\u001b[0;34m(what, tree1, avals1, tree2, avals2)\u001b[0m\n\u001b[1;32m   2195\u001b[0m     diff = tree_multimap(_show_diff, tree_unflatten(tree1, avals1),\n\u001b[1;32m   2196\u001b[0m                          tree_unflatten(tree2, avals2))\n\u001b[0;32m-> 2197\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{what} must have identical types, got\\n{diff}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: true_fun and false_fun output must have identical types, got\n((State(qp=QP(pos='ShapedArray(float32[16,4,3])', rot='ShapedArray(float32[16,4,4])', vel='ShapedArray(float32[16,4,3])', ang='ShapedArray(float32[16,4,3])'), obs='ShapedArray(float32[16,11])', reward='ShapedArray(float32[16])', done='ShapedArray(float32[16])', metrics={'rewardCtrl': 'ShapedArray(float32[16])', 'rewardDist': 'ShapedArray(float32[16])'}, info={'steps': 'ShapedArray(float32[16])', 'truncation': 'ShapedArray(float32[16])'}), 'ShapedArray(float32[16,11])', FrozenDict({\n    params: {\n        Dense_0: {\n            bias: 'ShapedArray(float32[64])',\n            kernel: 'ShapedArray(float32[11,64])',\n        },\n        Dense_1: {\n            bias: 'ShapedArray(float32[64])',\n            kernel: 'ShapedArray(float32[64,64])',\n        },\n        Dense_2: {\n            bias: 'ShapedArray(float32[2])',\n            kernel: 'ShapedArray(float32[64,2])',\n        },\n        GRUCell_0: {\n            hn: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            hr: {\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            hz: {\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            in: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            ir: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n            iz: {\n                bias: 'ShapedArray(float32[11])',\n                kernel: 'ShapedArray(float32[11,11])',\n            },\n        },\n    },\n}), ('ShapedArray(float32[])', 'ShapedArray(float32[11])', 'ShapedArray(float32[11])'), 'DIFFERENT ShapedArray(float32[], weak_type=True) vs. ShapedArray(float32[16])', 'ShapedArray(int32[], weak_type=True)'), None)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1):\n",
    "    key, noise_key, train_key = jax.random.split(key, num=3)\n",
    "\n",
    "    #policy_params_with_noise, noise = add_noise_pmap(policy_params, noise_std, noise_keys)\n",
    "    policy_params_with_noise, noise1, noise2 = add_uniform_and_pareto_noise(policy_params, noise_beta, noise_scale, noise_key)\n",
    "    \n",
    "    rewards_before, obs, acts, states_before = do_one_rollout(env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_key, episode_length, action_repeat, normalize_observations)\n",
    "    policy_params_trained, rewards_lists = do_local_apg(apg_epochs, env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_key, learning_rate, episode_length, action_repeat, normalize_observations, batch_size, clipping, truncation_length)\n",
    "    rewards_after, obs, acts, states_after = do_one_rollout(env_fn, policy.apply, normalizer_params, policy_params_trained, train_key, episode_length, action_repeat, normalize_observations)\n",
    "            \n",
    "    print(jnp.any(policy_params_trained['params']['Dense_1']['kernel'] - policy_params_with_noise['params']['Dense_1']['kernel']))\n",
    "    \n",
    "    top_idx = sorted(range(len(rewards_lists)), key=lambda k: jnp.mean(rewards_lists[k][-5:]), reverse=True)\n",
    "    \n",
    "    normalizer_params = obs_normalizer_update_fn(normalizer_params, obs[top_idx[0],:])\n",
    "    \n",
    "    _, params_def = jax.tree_flatten(policy_params)\n",
    "    params_flat, _ = jax.tree_flatten(policy_params_trained)\n",
    "    top_params_flat = [param[top_idx[0]] for param in params_flat]\n",
    "    top_params = jax.tree_unflatten(params_def, top_params_flat)\n",
    "    \n",
    "    \n",
    "#     _, norm_def = jax.tree_flatten(normalizer_params)\n",
    "#     norm_flat, _ = jax.tree_flatten(normalizer_params_all)\n",
    "#     top_norm_flat = [param[top_idx[0]] for param in norm_flat]\n",
    "#     top_norms = jax.tree_unflatten(norm_def, top_norm_flat)\n",
    "    \n",
    "    noise_beta -= .1\n",
    "    \n",
    "    if rewards_lists[top_idx[0]][-1] > best_reward:\n",
    "        noise_beta = 2.0\n",
    "        policy_params = top_params\n",
    "        top_normalizer_params = normalizer_params\n",
    "        best_reward = jnp.mean(rewards_lists[top_idx[0]][-5:])\n",
    "        \n",
    "    meta_rewards_list.append(best_reward)\n",
    "    \n",
    "    print(f\" Iteration {i} --------------------------------\")\n",
    "    \n",
    "    for j in range(len(top_idx)):\n",
    "        done_idx = jnp.where(states_before.done[top_idx[j], :], size=1)[0].item()\n",
    "        if done_idx == 0:\n",
    "            done_idx = rewards_before.shape[-1]\n",
    "        rewards_sum_before = jnp.sum(rewards_before[top_idx[j],:done_idx])\n",
    "\n",
    "        done_idx = jnp.where(states_after.done[top_idx[j], :], size=1)[0].item()\n",
    "        if done_idx == 0:\n",
    "            done_idx = rewards_after.shape[-1]\n",
    "        rewards_sum_after = jnp.sum(rewards_after[top_idx[j],:done_idx])\n",
    "        \n",
    "        print(f\"{j} : reward: {rewards_sum_before} -> {jnp.mean(rewards_lists[top_idx[j]][-5:])}  |  {rewards_sum_after}\")\n",
    "\n",
    "    \n",
    "    #print(f\"{i} : best reward: {rewards_sum_before} -> {rewards_lists[top_idx[0]][-1]}  |  {rewards_sum_after}\")\n",
    "\n",
    "    print(\"Best reward so far: \", best_reward)\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.any(policy_params_trained['params']['Dense_1']['kernel'] - policy_params_with_noise['params']['Dense_1']['kernel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brax.jumpy as jp\n",
    "@jax.jit\n",
    "def do_rnn_rollout(policy_params, normalizer_params, key):\n",
    "    init_state = env.reset(key)\n",
    "    h0 = jp.zeros_like(init_state.obs)\n",
    "\n",
    "    def do_one_rnn_step(carry, step_idx):\n",
    "        state, h, policy_params, normalizer_params  = carry\n",
    "\n",
    "        normed_obs = obs_normalizer_apply_fn(normalizer_params, state.obs)\n",
    "        h1 , actions = policy.apply(policy_params, h, normed_obs)\n",
    "        #actions = jp.ones_like(actions)*0.0\n",
    "        nstate = env.step(state, actions)    \n",
    "        #h1 = jax.lax.cond(nstate.done, lambda x: jnp.zeros_like(h1), lambda x: h1, None)\n",
    "        return (jax.lax.stop_gradient(nstate), h1, policy_params, normalizer_params), (nstate.reward,state.obs, actions, nstate)\n",
    "\n",
    "\n",
    "    _, (rewards, obs, acts, states) = jp.scan(\n",
    "        do_one_rnn_step, (init_state, h0, policy_params, normalizer_params),\n",
    "        (jnp.array(range(episode_length // action_repeat))),\n",
    "        length=episode_length // action_repeat)\n",
    "\n",
    "    return rewards, obs, acts, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, reset_key = jax.random.split(key)\n",
    "(rewards, obs, acts, states) = do_rnn_rollout(policy_params, top_normalizer_params, reset_key)\n",
    "\n",
    "done_idx = jnp.where(states.done, size=1)[0].item()\n",
    "rewards_sum = jnp.sum(rewards[:done_idx])\n",
    "\n",
    "plt.plot(obs[:done_idx,:]);\n",
    "plt.figure()\n",
    "plt.plot(acts);\n",
    "print(rewards_sum)\n",
    "print(states.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, obs, acts, states = do_one_rollout(env_fn, policy.apply, top_normalizer_params, top_params, key, episode_length, action_repeat, normalize_observations)\n",
    "print(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_flat, qp_def = jax.tree_flatten(states.qp)\n",
    "\n",
    "qp_list = []\n",
    "\n",
    "for i in range(qp_flat[0].shape[0]):\n",
    "    qpc=[]\n",
    "    for thing in qp_flat:\n",
    "        qpc.append(thing[i,:])\n",
    "    qp_list.append(jax.tree_unflatten(qp_def, qpc))\n",
    "    \n",
    "\n",
    "visualize(env.sys, qp_list, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brax",
   "language": "python",
   "name": "brax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
