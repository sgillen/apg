{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=12'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "from brax.training import normalization\n",
    "\n",
    "\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.envs import create_fn\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from controllers import GruController, MlpController, LinearController\n",
    "from common import do_local_apg, add_guassian_noise, add_uniform_noise, add_uniform_and_pareto_noise, add_sym_pareto_noise, do_one_rollout\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "def visualize(sys, qps, height=480):\n",
    "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
    "  return HTML(html.render(sys, qps, height=height))\n",
    "\n",
    "len(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length = 1000\n",
    "action_repeat = 1\n",
    "batch_size = jax.local_device_count()\n",
    "starting_noise_std = 0.02\n",
    "\n",
    "noise_scale = 3.0\n",
    "noise_beta = 2.0\n",
    "\n",
    "\n",
    "apg_epochs = 50\n",
    "batch_size = 1\n",
    "truncation_length = None\n",
    "learning_rate = 5e-4\n",
    "clipping = 1e9\n",
    "\n",
    "normalize_observations=True\n",
    "\n",
    "env_name = \"inverted_double_pendulum\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=None, auto_reset=False)\n",
    "env = env_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "reset_keys = jax.random.split(key, num=jax.local_device_count())\n",
    "_, model_key = jax.random.split(reset_keys[0])\n",
    "noise_keys = jax.random.split(model_key, num=jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy = LinearController(env.observation_size,env.action_size)\n",
    "#policy = GruController(env.observation_size, env.action_size, 32)\n",
    "\n",
    "policy = GruController(env.observation_size, env.action_size, 32)\n",
    "\n",
    "normalizer_params, obs_normalizer_update_fn, obs_normalizer_apply_fn = normalization.create_observation_normalizer(\n",
    "          env.observation_size, normalize_observations, num_leading_batch_dims=1)\n",
    "\n",
    "add_noise_pmap = jax.pmap(add_guassian_noise, in_axes=(None,None,0))\n",
    "add_pareto_noise_pmap = jax.pmap(add_uniform_and_pareto_noise, in_axes=(None,None,None,0))\n",
    "\n",
    "do_apg_pmap = jax.pmap(do_local_apg, in_axes = (None,None,None,None,0,0,None,None,None,None,None,None), static_broadcasted_argnums=(0,1,2,6,7,8,9,10,11,12))\n",
    "do_rollout_pmap = jax.pmap(do_one_rollout, in_axes = (None,None,None,0,0,None,None,None), static_broadcasted_argnums=(0,1,5,6,7))\n",
    "\n",
    "\n",
    "init_states = jax.pmap(env.reset)(reset_keys)\n",
    "x0 = init_states.obs\n",
    "h0 = jnp.zeros(env.observation_size)\n",
    "\n",
    "policy_params = policy.init(model_key, h0, x0)\n",
    "\n",
    "# policy_params = policy_params.unfreeze()\n",
    "# policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[0].set(0.0)\n",
    "# policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[1].set(0.0)\n",
    "# policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[2].set(0.0)\n",
    "# policy_params['params']['Dense_0']['kernel'] = policy_params['params']['Dense_0']['kernel'].at[3].set(0.0)\n",
    "# policy_params = flax.core.frozen_dict.FrozenDict(policy_params)\n",
    "\n",
    "best_reward = -float('inf')\n",
    "meta_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " Iteration 0 --------------------------------\n",
      "0 : reward: -7677076.5 -> 6540.2734375  |  6631.6806640625\n",
      "1 : reward: -6714848.5 -> 6369.2626953125  |  6368.39453125\n",
      "2 : reward: -8442754.0 -> 6368.4541015625  |  6367.1083984375\n",
      "3 : reward: -7304627.0 -> 6337.4228515625  |  6336.896484375\n",
      "4 : reward: -5328421.0 -> 6265.9775390625  |  6264.76220703125\n",
      "5 : reward: -8525818.0 -> 6230.86767578125  |  6233.90087890625\n",
      "6 : reward: -10810663.0 -> 6048.47900390625  |  6053.95068359375\n",
      "7 : reward: -5849679.0 -> -2670.885986328125  |  -686.8900146484375\n",
      "8 : reward: -6479156.0 -> -45733.52734375  |  -51991.07421875\n",
      "9 : reward: -3545922.5 -> -490410.5  |  -548191.5\n",
      "10 : reward: -6975757.0 -> -2402929.25  |  -2104960.75\n",
      "11 : reward: -2595340.25 -> -3157927.25  |  -3126185.75\n",
      "Best reward so far:  6540.2734\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 1 --------------------------------\n",
      "0 : reward: 6586.60693359375 -> 7280.80322265625  |  7141.50048828125\n",
      "1 : reward: 6385.6103515625 -> 7058.6376953125  |  7194.91796875\n",
      "2 : reward: 7028.9072265625 -> 6945.07958984375  |  6935.1640625\n",
      "3 : reward: 5487.330078125 -> 6940.9296875  |  6893.513671875\n",
      "4 : reward: -158427.875 -> 6810.42822265625  |  6833.92578125\n",
      "5 : reward: 7309.568359375 -> 6725.64013671875  |  6785.46923828125\n",
      "6 : reward: 7226.015625 -> 6713.125  |  6821.49609375\n",
      "7 : reward: 6455.4814453125 -> 6396.97509765625  |  6387.0302734375\n",
      "8 : reward: 6311.3896484375 -> 6262.4501953125  |  6251.78515625\n",
      "9 : reward: 7238.90625 -> 6186.27099609375  |  6187.2822265625\n",
      "10 : reward: 7217.08203125 -> 6060.1669921875  |  6041.828125\n",
      "11 : reward: -1895.82177734375 -> 5232.69091796875  |  5807.3349609375\n",
      "Best reward so far:  7280.803\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 2 --------------------------------\n",
      "0 : reward: -83187.375 -> 6961.72802734375  |  6853.60986328125\n",
      "1 : reward: -101793.828125 -> 6611.25341796875  |  6413.07958984375\n",
      "2 : reward: 6174.80029296875 -> 6341.81787109375  |  6322.66259765625\n",
      "3 : reward: -1941.3458251953125 -> 6243.57470703125  |  6232.609375\n",
      "4 : reward: 5493.3388671875 -> 6174.615234375  |  6168.21240234375\n",
      "5 : reward: 7247.16552734375 -> 6174.0078125  |  6167.892578125\n",
      "6 : reward: -48813.71484375 -> 6173.1240234375  |  6167.19970703125\n",
      "7 : reward: 1206.1834716796875 -> 6168.70458984375  |  6163.50537109375\n",
      "8 : reward: -388148.125 -> 6167.76123046875  |  6161.9072265625\n",
      "9 : reward: -3014.289794921875 -> 6167.505859375  |  6158.482421875\n",
      "10 : reward: -2559.0419921875 -> 6162.90576171875  |  6156.00048828125\n",
      "11 : reward: -30695.515625 -> 6137.93408203125  |  6082.6728515625\n",
      "Best reward so far:  7280.803\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 3 --------------------------------\n",
      "0 : reward: -53900.03515625 -> 6693.88134765625  |  6711.5966796875\n",
      "1 : reward: 7216.302734375 -> 6602.76416015625  |  6641.66650390625\n",
      "2 : reward: -5585.4541015625 -> 6501.4521484375  |  6478.1728515625\n",
      "3 : reward: -190618.140625 -> 6280.70166015625  |  6351.748046875\n",
      "4 : reward: 2323.0126953125 -> 6174.359375  |  6168.1220703125\n",
      "5 : reward: -17294.45703125 -> 6172.08984375  |  6155.171875\n",
      "6 : reward: -79081.5625 -> 6166.52197265625  |  6160.5556640625\n",
      "7 : reward: 3644.48193359375 -> 6151.29296875  |  6150.458984375\n",
      "8 : reward: 3712.62158203125 -> 6115.2119140625  |  6115.8779296875\n",
      "9 : reward: -22996.447265625 -> 6009.7802734375  |  6006.8544921875\n",
      "10 : reward: -9548.521484375 -> 5699.19677734375  |  5936.888671875\n",
      "11 : reward: -85799.3828125 -> -2341221.5  |  -2333275.75\n",
      "Best reward so far:  7280.803\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 4 --------------------------------\n",
      "0 : reward: 7017.119140625 -> 6699.09521484375  |  6692.7021484375\n",
      "1 : reward: 6168.931640625 -> 6560.44384765625  |  6496.4052734375\n",
      "2 : reward: 69.88034057617188 -> 6462.75  |  6454.07373046875\n",
      "3 : reward: 7268.6064453125 -> 6350.29833984375  |  7351.125\n",
      "4 : reward: -23969.87109375 -> 6224.71044921875  |  6194.1416015625\n",
      "5 : reward: -2080.328125 -> 6167.30810546875  |  6161.4853515625\n",
      "6 : reward: -14828.7783203125 -> 6129.505859375  |  6123.734375\n",
      "7 : reward: 6660.6181640625 -> 5588.8017578125  |  5589.166015625\n",
      "8 : reward: 6388.4755859375 -> -14903.603515625  |  -7208.81298828125\n",
      "9 : reward: 3989.11181640625 -> -34116.51953125  |  -32185.48046875\n",
      "10 : reward: -471598.4375 -> -69128.8125  |  1971.75048828125\n",
      "11 : reward: -669725.9375 -> -112837.9921875  |  5844.94921875\n",
      "Best reward so far:  7280.803\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 5 --------------------------------\n",
      "0 : reward: 6460.85009765625 -> 7210.4921875  |  6814.1357421875\n",
      "1 : reward: -1341.19384765625 -> 7093.66748046875  |  7137.46337890625\n",
      "2 : reward: -1076.5633544921875 -> 6879.32763671875  |  6902.24462890625\n",
      "3 : reward: -4756.9287109375 -> 6853.55615234375  |  6951.15966796875\n",
      "4 : reward: 4517.49169921875 -> 6672.1103515625  |  6455.623046875\n",
      "5 : reward: 6008.3984375 -> 6462.51806640625  |  6465.9853515625\n",
      "6 : reward: -132.28256225585938 -> 6251.83154296875  |  6257.5009765625\n",
      "7 : reward: 5498.689453125 -> 6174.30224609375  |  6168.2060546875\n",
      "8 : reward: -43803.078125 -> 6036.822265625  |  6031.068359375\n",
      "9 : reward: -705656.875 -> 4819.70166015625  |  5843.0498046875\n",
      "10 : reward: 2160.47998046875 -> -7680.0  |  -12478.58984375\n",
      "11 : reward: -232050.375 -> -3840505.75  |  -3840716.5\n",
      "Best reward so far:  7280.803\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 6 --------------------------------\n",
      "0 : reward: -3547781.0 -> 7335.28857421875  |  7226.57080078125\n",
      "1 : reward: -1119202.5 -> 6431.65234375  |  6494.0205078125\n",
      "2 : reward: -25591.9765625 -> 6218.94482421875  |  6219.796875\n",
      "3 : reward: -70771.703125 -> 6169.28564453125  |  6163.2373046875\n",
      "4 : reward: 7062.5478515625 -> 6154.26171875  |  6142.1904296875\n",
      "5 : reward: -183002.375 -> 1411.3785400390625  |  1412.3004150390625\n",
      "6 : reward: 6240.3876953125 -> -4337.97998046875  |  -20859.810546875\n",
      "7 : reward: -27399.931640625 -> -6219.64892578125  |  -2822.099853515625\n",
      "8 : reward: -1918.6756591796875 -> -87457.1171875  |  -90365.890625\n",
      "9 : reward: -4366641.5 -> -835195.6875  |  -1466689.75\n",
      "10 : reward: 5870.560546875 -> -1930039.625  |  -1956395.375\n",
      "11 : reward: -2858923.5 -> -2225298.75  |  -3418388.25\n",
      "Best reward so far:  7335.2886\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 7 --------------------------------\n",
      "0 : reward: -30351.9140625 -> 7437.37646484375  |  7480.34912109375\n",
      "1 : reward: 7456.982421875 -> 7402.15478515625  |  7503.47412109375\n",
      "2 : reward: 6443.615234375 -> 7263.1533203125  |  7198.2568359375\n",
      "3 : reward: -177630.296875 -> 7185.13134765625  |  7216.8173828125\n",
      "4 : reward: 3648.548095703125 -> 6915.34716796875  |  6131.73681640625\n",
      "5 : reward: 6154.48388671875 -> 6636.17431640625  |  6675.62890625\n",
      "6 : reward: 7628.76953125 -> 6356.025390625  |  6348.5263671875\n",
      "7 : reward: -168285.328125 -> 6285.0732421875  |  6250.732421875\n",
      "8 : reward: 6296.07275390625 -> 6188.44091796875  |  6155.16259765625\n",
      "9 : reward: -299278.875 -> 6078.77587890625  |  6075.642578125\n",
      "10 : reward: 6497.2353515625 -> -292.00469970703125  |  272.3323059082031\n",
      "11 : reward: -43418.0390625 -> -56874.40234375  |  -48434.8203125\n",
      "Best reward so far:  7437.3765\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 8 --------------------------------\n",
      "0 : reward: 7220.19580078125 -> 7400.10009765625  |  7407.119140625\n",
      "1 : reward: 7445.3857421875 -> 7205.63671875  |  7204.5380859375\n",
      "2 : reward: 6177.95556640625 -> 6957.98974609375  |  6962.96044921875\n",
      "3 : reward: 5998.17919921875 -> 6935.31396484375  |  6923.54345703125\n",
      "4 : reward: 6289.650390625 -> 6925.37451171875  |  6945.6298828125\n",
      "5 : reward: 7241.6474609375 -> 6651.34716796875  |  6649.0400390625\n",
      "6 : reward: 6822.5859375 -> 6533.53955078125  |  6559.876953125\n",
      "7 : reward: 5960.94677734375 -> 6295.91357421875  |  6291.20263671875\n",
      "8 : reward: 6835.5478515625 -> 6261.99951171875  |  6268.95556640625\n",
      "9 : reward: 6603.8046875 -> 5830.62451171875  |  5819.26708984375\n",
      "10 : reward: 3724.1796875 -> -864126.5  |  -624486.875\n",
      "11 : reward: 6566.384765625 -> -888221.8125  |  -720193.125\n",
      "Best reward so far:  7437.3765\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 9 --------------------------------\n",
      "0 : reward: 6178.763671875 -> 7507.7470703125  |  7357.826171875\n",
      "1 : reward: 6357.1806640625 -> 7394.83447265625  |  7338.865234375\n",
      "2 : reward: 6386.08154296875 -> 7178.61865234375  |  6959.640625\n",
      "3 : reward: 6166.04541015625 -> 7001.8515625  |  6822.43359375\n",
      "4 : reward: 6314.43212890625 -> 6989.24365234375  |  6710.8330078125\n",
      "5 : reward: 5736.10546875 -> 6905.02978515625  |  6765.74658203125\n",
      "6 : reward: 6966.72900390625 -> 6531.16943359375  |  6501.095703125\n",
      "7 : reward: 6168.806640625 -> 6274.75732421875  |  6281.61767578125\n",
      "8 : reward: 6414.0576171875 -> 6171.857421875  |  6165.65234375\n",
      "9 : reward: 6717.62060546875 -> 5817.74755859375  |  5826.36328125\n",
      "10 : reward: 6169.294921875 -> 5713.9111328125  |  6303.154296875\n",
      "11 : reward: -13122.7431640625 -> -348532.96875  |  -338362.40625\n",
      "Best reward so far:  7507.747\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 10 --------------------------------\n",
      "0 : reward: 7018.08642578125 -> 7826.61279296875  |  7812.904296875\n",
      "1 : reward: 6722.36181640625 -> 7088.29541015625  |  6965.9306640625\n",
      "2 : reward: 6596.31103515625 -> 7073.921875  |  7043.56201171875\n",
      "3 : reward: 6913.0146484375 -> 7071.48681640625  |  7064.49365234375\n",
      "4 : reward: 7204.97265625 -> 6957.42919921875  |  6939.80078125\n",
      "5 : reward: 6738.0244140625 -> 6956.33935546875  |  7032.77099609375\n",
      "6 : reward: 7393.02734375 -> 6806.625  |  6790.4697265625\n",
      "7 : reward: 7087.9150390625 -> 6796.23583984375  |  7812.8974609375\n",
      "8 : reward: 7108.3408203125 -> 6394.69921875  |  6368.9267578125\n",
      "9 : reward: 7290.74755859375 -> 6162.9990234375  |  6159.37841796875\n",
      "10 : reward: 6688.85546875 -> 5523.169921875  |  7469.89111328125\n",
      "11 : reward: 6973.4248046875 -> 2718.837646484375  |  4413.99755859375\n",
      "Best reward so far:  7826.613\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 11 --------------------------------\n",
      "0 : reward: 7239.837890625 -> 7853.36279296875  |  7869.591796875\n",
      "1 : reward: 7282.2490234375 -> 7731.16552734375  |  7982.84814453125\n",
      "2 : reward: 6289.3837890625 -> 7659.13427734375  |  7647.8994140625\n",
      "3 : reward: 1918.332275390625 -> 7624.07177734375  |  7634.2392578125\n",
      "4 : reward: 7577.8876953125 -> 7569.31494140625  |  7611.0126953125\n",
      "5 : reward: 7152.3359375 -> 7552.3916015625  |  7549.4208984375\n",
      "6 : reward: 5871.3642578125 -> 7295.6728515625  |  7296.6962890625\n",
      "7 : reward: -10031.15625 -> 7280.23583984375  |  7253.2998046875\n",
      "8 : reward: 6866.2685546875 -> 7232.63134765625  |  7267.0166015625\n",
      "9 : reward: 7451.447265625 -> 6321.82373046875  |  6377.9404296875\n",
      "10 : reward: 5218.68359375 -> 5667.3857421875  |  6791.390625\n",
      "11 : reward: 6846.39208984375 -> 299.1086120605469  |  330.385009765625\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 12 --------------------------------\n",
      "0 : reward: 6642.10888671875 -> 7397.6259765625  |  7396.21533203125\n",
      "1 : reward: 7484.14453125 -> 7379.94677734375  |  7393.29296875\n",
      "2 : reward: -1223.2469482421875 -> 7198.51416015625  |  7309.41064453125\n",
      "3 : reward: 7027.2109375 -> 7051.60791015625  |  7052.1025390625\n",
      "4 : reward: 7588.591796875 -> 7001.1259765625  |  6992.77099609375\n",
      "5 : reward: 7457.72265625 -> 6893.9306640625  |  7003.34619140625\n",
      "6 : reward: 7326.78515625 -> 6792.8408203125  |  6693.2626953125\n",
      "7 : reward: 7248.8701171875 -> 6567.4765625  |  6774.39453125\n",
      "8 : reward: 6503.6220703125 -> 6150.6767578125  |  4344.099609375\n",
      "9 : reward: 6329.767578125 -> 5783.16552734375  |  6007.9794921875\n",
      "10 : reward: 7648.705078125 -> -2157.20703125  |  6419.76953125\n",
      "11 : reward: 3603.687255859375 -> -24624.521484375  |  -16142.3564453125\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 13 --------------------------------\n",
      "0 : reward: 6952.0244140625 -> 7524.14697265625  |  7252.7880859375\n",
      "1 : reward: 7419.7822265625 -> 7436.0048828125  |  7477.82373046875\n",
      "2 : reward: 6962.162109375 -> 7239.06884765625  |  7203.0166015625\n",
      "3 : reward: 377.94842529296875 -> 6774.22216796875  |  6329.61767578125\n",
      "4 : reward: 674.689208984375 -> 6412.12939453125  |  6392.361328125\n",
      "5 : reward: 1218.211181640625 -> 6403.72412109375  |  6461.4970703125\n",
      "6 : reward: 979.5902099609375 -> 6346.54931640625  |  4090.0654296875\n",
      "7 : reward: 6885.404296875 -> 6291.45556640625  |  6146.94140625\n",
      "8 : reward: 7108.5390625 -> 6210.7578125  |  6415.0439453125\n",
      "9 : reward: -8696.568359375 -> 5852.43994140625  |  6322.91064453125\n",
      "10 : reward: -6028.474609375 -> 4991.9375  |  6705.0009765625\n",
      "11 : reward: 3203.99951171875 -> 4394.39453125  |  4108.1904296875\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 14 --------------------------------\n",
      "0 : reward: 6155.12353515625 -> 7795.6533203125  |  7819.5068359375\n",
      "1 : reward: 4907.6689453125 -> 7589.03515625  |  7632.01806640625\n",
      "2 : reward: 6848.5068359375 -> 7371.20068359375  |  7255.697265625\n",
      "3 : reward: 7472.16796875 -> 7291.02294921875  |  7285.0966796875\n",
      "4 : reward: -6256.7216796875 -> 7125.0703125  |  7186.38671875\n",
      "5 : reward: 7390.537109375 -> 7074.06884765625  |  7092.609375\n",
      "6 : reward: 6554.2880859375 -> 6671.9697265625  |  6616.0\n",
      "7 : reward: 6453.5380859375 -> 6562.6572265625  |  6614.16796875\n",
      "8 : reward: -2429.03759765625 -> 6313.97265625  |  6972.2177734375\n",
      "9 : reward: 6925.11328125 -> 5803.50634765625  |  5957.9384765625\n",
      "10 : reward: 5667.728515625 -> -2524.2529296875  |  6114.8544921875\n",
      "11 : reward: 6340.71044921875 -> -3402425.25  |  -3396669.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 15 --------------------------------\n",
      "0 : reward: 6771.7197265625 -> 7803.71240234375  |  7913.21630859375\n",
      "1 : reward: 6745.18359375 -> 7643.33740234375  |  7716.5126953125\n",
      "2 : reward: 6111.89453125 -> 7444.10009765625  |  7507.5693359375\n",
      "3 : reward: 2054.7451171875 -> 7222.96044921875  |  7208.1943359375\n",
      "4 : reward: 7269.6923828125 -> 6990.56884765625  |  7200.5625\n",
      "5 : reward: -95073.6171875 -> 6826.88623046875  |  6830.6181640625\n",
      "6 : reward: 7040.9970703125 -> 6576.95166015625  |  6555.09033203125\n",
      "7 : reward: -170.05703735351562 -> 941.3857421875  |  1094.25244140625\n",
      "8 : reward: -104703.703125 -> -260.22662353515625  |  3650.07177734375\n",
      "9 : reward: 3925.9931640625 -> -20806.076171875  |  -106779.46875\n",
      "10 : reward: -475413.5 -> -155971.6875  |  -144962.234375\n",
      "11 : reward: 6471.533203125 -> -4522649.5  |  -6211133.5\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 16 --------------------------------\n",
      "0 : reward: 7152.30859375 -> 7823.90966796875  |  7845.6025390625\n",
      "1 : reward: 4963.97509765625 -> 7284.30322265625  |  7133.4765625\n",
      "2 : reward: 6010.92578125 -> 7030.6572265625  |  7057.7568359375\n",
      "3 : reward: 6338.8515625 -> 7028.51123046875  |  7000.3603515625\n",
      "4 : reward: 6151.7900390625 -> 6913.86328125  |  6828.99072265625\n",
      "5 : reward: -2689.9287109375 -> 5977.62158203125  |  5984.3818359375\n",
      "6 : reward: -2633970.5 -> -304.6629943847656  |  6198.46142578125\n",
      "7 : reward: -21295.484375 -> -11115.595703125  |  -21278.3203125\n",
      "8 : reward: -5075.7275390625 -> -14742.6005859375  |  -14423.208984375\n",
      "9 : reward: 6437.54150390625 -> -107173.0234375  |  -105696.625\n",
      "10 : reward: -24931.560546875 -> -504769.09375  |  -1297459.25\n",
      "11 : reward: 282.92205810546875 -> -15577680.0  |  -15673616.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 17 --------------------------------\n",
      "0 : reward: -167161.25 -> 7533.67822265625  |  7530.1875\n",
      "1 : reward: 6136.14453125 -> 7251.51318359375  |  7134.9248046875\n",
      "2 : reward: 6172.0068359375 -> 7224.81591796875  |  7384.80615234375\n",
      "3 : reward: -6728363.5 -> 6770.15771484375  |  6773.2529296875\n",
      "4 : reward: 6167.8603515625 -> 6167.0068359375  |  6160.4990234375\n",
      "5 : reward: -646415.0625 -> 6106.1064453125  |  6105.1259765625\n",
      "6 : reward: 3187.05029296875 -> 2723.4140625  |  3335.68115234375\n",
      "7 : reward: -76866.40625 -> 1207.7088623046875  |  582.188720703125\n",
      "8 : reward: 3590.85693359375 -> -14057.6689453125  |  -57396.3203125\n",
      "9 : reward: 2334.7685546875 -> -56316.3125  |  -67311.046875\n",
      "10 : reward: -1697180.5 -> -427109.84375  |  -319632.3125\n",
      "11 : reward: -409520.1875 -> -4465625.5  |  -4202570.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 18 --------------------------------\n",
      "0 : reward: 6182.4951171875 -> 7144.35498046875  |  7339.1552734375\n",
      "1 : reward: 7496.279296875 -> 7024.15478515625  |  7011.349609375\n",
      "2 : reward: -6177.9189453125 -> 6892.82421875  |  6910.7060546875\n",
      "3 : reward: 6157.013671875 -> 6155.94140625  |  6149.8173828125\n",
      "4 : reward: -65532.4453125 -> 4340.30419921875  |  4402.3408203125\n",
      "5 : reward: -153966.484375 -> -7631.22998046875  |  -6749.95556640625\n",
      "6 : reward: 3237.036865234375 -> -21751.625  |  -1871.25634765625\n",
      "7 : reward: -151142.34375 -> -22160.990234375  |  -21416.98046875\n",
      "8 : reward: -824042.9375 -> -174469.421875  |  -159661.5\n",
      "9 : reward: -1275599.5 -> -993182.6875  |  -939249.125\n",
      "10 : reward: -79645.328125 -> -1169374.75  |  -1195455.25\n",
      "11 : reward: -6557343.0 -> -3677077.75  |  -3860825.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 19 --------------------------------\n",
      "0 : reward: 6148.53515625 -> 7101.74560546875  |  7192.84521484375\n",
      "1 : reward: -12332098.0 -> 6808.01318359375  |  6853.85546875\n",
      "2 : reward: 6447.099609375 -> 6687.2783203125  |  6673.931640625\n",
      "3 : reward: -18153.123046875 -> 6215.03369140625  |  6207.259765625\n",
      "4 : reward: -62043.26953125 -> -16.980863571166992  |  473.26971435546875\n",
      "5 : reward: -46633.5078125 -> -6543.8603515625  |  -4741.849609375\n",
      "6 : reward: -37043.4296875 -> -13756.705078125  |  -13359.310546875\n",
      "7 : reward: 7673.9619140625 -> -15076.455078125  |  -76213.2421875\n",
      "8 : reward: -16353.1123046875 -> -235989.953125  |  -209452.125\n",
      "9 : reward: 6273.21875 -> -608266.0  |  -570023.625\n",
      "10 : reward: -4813632.5 -> -749902.4375  |  -830433.5\n",
      "11 : reward: -1914846.25 -> -2330290.0  |  -1986545.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 20 --------------------------------\n",
      "0 : reward: 725.29931640625 -> 7335.94677734375  |  7337.68115234375\n",
      "1 : reward: -31572.73828125 -> 6480.84130859375  |  6222.9140625\n",
      "2 : reward: 3021.272216796875 -> 6207.927734375  |  6201.4052734375\n",
      "3 : reward: -515623.5625 -> 6127.03369140625  |  6291.60107421875\n",
      "4 : reward: -987421.5 -> 5707.38232421875  |  5710.9482421875\n",
      "5 : reward: -1226348.125 -> 4705.3837890625  |  5596.78076171875\n",
      "6 : reward: -153839.625 -> -1558.996337890625  |  -819.2066040039062\n",
      "7 : reward: -304648.28125 -> -2490.8359375  |  7162.05419921875\n",
      "8 : reward: -83849.3828125 -> -95958.0546875  |  -83495.8203125\n",
      "9 : reward: -31321336.0 -> -6381663.5  |  -6005008.0\n",
      "10 : reward: -449198.8125 -> -7031749.0  |  -6905037.0\n",
      "11 : reward: -16399876.0 -> -7972993.5  |  -9257113.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 21 --------------------------------\n",
      "0 : reward: -15552285.0 -> 6283.8134765625  |  6271.0654296875\n",
      "1 : reward: 6314.580078125 -> 6202.30810546875  |  6176.572265625\n",
      "2 : reward: -13843.822265625 -> 6130.17529296875  |  6132.2822265625\n",
      "3 : reward: -2262630.5 -> 6099.51513671875  |  6087.77587890625\n",
      "4 : reward: 6462.779296875 -> 5026.2197265625  |  5718.4169921875\n",
      "5 : reward: -9037.18359375 -> 1042.7337646484375  |  1042.806884765625\n",
      "6 : reward: -13354.09375 -> -21760.08203125  |  -30871.94921875\n",
      "7 : reward: -6810060.0 -> -32950.86328125  |  -31588.720703125\n",
      "8 : reward: -38506.05078125 -> -61496.40625  |  -3073.596435546875\n",
      "9 : reward: -18278474.0 -> -92222.5703125  |  -80925.3984375\n",
      "10 : reward: -20170.02734375 -> -163041.0  |  -110713.671875\n",
      "11 : reward: -22737812.0 -> -22095798.0  |  -23337144.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 22 --------------------------------\n",
      "0 : reward: 5732.89453125 -> 7240.6376953125  |  7237.6103515625\n",
      "1 : reward: -6128.5546875 -> 6990.5234375  |  6988.9677734375\n",
      "2 : reward: -93860.953125 -> 6644.54638671875  |  6645.24951171875\n",
      "3 : reward: 7173.67626953125 -> 6184.97802734375  |  6186.88330078125\n",
      "4 : reward: -9869614.0 -> 6165.978515625  |  6156.12255859375\n",
      "5 : reward: 5432.283203125 -> 5831.6181640625  |  5828.0087890625\n",
      "6 : reward: -23473688.0 -> -1742.1771240234375  |  -30523.490234375\n",
      "7 : reward: -17384012.0 -> -29121.962890625  |  -22606.953125\n",
      "8 : reward: -565610.75 -> -342621.875  |  4059.669921875\n",
      "9 : reward: -1401413.125 -> -937277.1875  |  -931012.6875\n",
      "10 : reward: -29771.40234375 -> -3565515.25  |  -3547029.5\n",
      "11 : reward: -238613.75 -> -8429911.0  |  -8996852.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 23 --------------------------------\n",
      "0 : reward: 7635.203125 -> 7587.81591796875  |  6988.8125\n",
      "1 : reward: -252983.21875 -> 6211.2412109375  |  6227.7373046875\n",
      "2 : reward: -35353.51171875 -> 6157.708984375  |  6155.59912109375\n",
      "3 : reward: -27828464.0 -> 6069.08154296875  |  6015.57568359375\n",
      "4 : reward: -1304755.0 -> -16249.587890625  |  -26911.6953125\n",
      "5 : reward: -374851.125 -> -52993.29296875  |  -53540.3125\n",
      "6 : reward: 5191.9970703125 -> -198872.4375  |  171.8475341796875\n",
      "7 : reward: -166422.0 -> -267529.90625  |  -148287.296875\n",
      "8 : reward: -2906.6025390625 -> -3069464.75  |  -2863269.0\n",
      "9 : reward: -19949.8828125 -> -8317299.5  |  -8211246.0\n",
      "10 : reward: -2543323.25 -> -18982554.0  |  -19312680.0\n",
      "11 : reward: -50999364.0 -> -39537004.0  |  -39330016.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 24 --------------------------------\n",
      "0 : reward: -388195.75 -> 7128.07958984375  |  7150.18359375\n",
      "1 : reward: -739996.75 -> 6256.03173828125  |  6255.6396484375\n",
      "2 : reward: -10433728.0 -> -4615.67919921875  |  -4559.75439453125\n",
      "3 : reward: -536122.625 -> -18389.341796875  |  -22800.9140625\n",
      "4 : reward: -17224772.0 -> -53933.96484375  |  -71441.640625\n",
      "5 : reward: -1537.890625 -> -2300761.25  |  -2301687.0\n",
      "6 : reward: -32949648.0 -> -3172677.75  |  -6844207.0\n",
      "7 : reward: -6204982.0 -> -4473109.5  |  -5055817.5\n",
      "8 : reward: -6967810.0 -> -5184965.5  |  -7907995.0\n",
      "9 : reward: -2757024.75 -> -6496237.0  |  -6410158.0\n",
      "10 : reward: -1793044.25 -> -24790378.0  |  -24753376.0\n",
      "11 : reward: -40656480.0 -> -31001834.0  |  -30456348.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 25 --------------------------------\n",
      "0 : reward: 5953.3662109375 -> 6107.427734375  |  6100.07275390625\n",
      "1 : reward: -7374788.0 -> 4132.04052734375  |  2574.80615234375\n",
      "2 : reward: -351429.875 -> -118542.6640625  |  -116963.6171875\n",
      "3 : reward: -25011840.0 -> -1320775.25  |  -1086937.0\n",
      "4 : reward: -2905417.0 -> -2187481.25  |  -827512.0625\n",
      "5 : reward: -4144509.5 -> -4564673.5  |  -4495670.0\n",
      "6 : reward: -112639.4296875 -> -5285982.5  |  -8273620.0\n",
      "7 : reward: -534735.875 -> -10466698.0  |  -10815819.0\n",
      "8 : reward: -35539276.0 -> -21244144.0  |  -21278608.0\n",
      "9 : reward: -6881120.0 -> -30154164.0  |  -30109442.0\n",
      "10 : reward: -48932908.0 -> -43966308.0  |  -43644960.0\n",
      "11 : reward: -54863544.0 -> -53096356.0  |  -53287752.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 26 --------------------------------\n",
      "0 : reward: -3010100.25 -> 6464.0390625  |  6422.7880859375\n",
      "1 : reward: 7112.88818359375 -> 6303.9970703125  |  6357.75927734375\n",
      "2 : reward: -104393.125 -> -36408.8125  |  -36561.24609375\n",
      "3 : reward: -145723.9375 -> -709682.5  |  -676317.0\n",
      "4 : reward: -6215950.5 -> -4410641.0  |  -3788619.25\n",
      "5 : reward: -51846332.0 -> -4854978.5  |  -6428539.5\n",
      "6 : reward: -22582948.0 -> -4864888.0  |  -4808844.0\n",
      "7 : reward: -22004622.0 -> -9227578.0  |  -8925932.0\n",
      "8 : reward: -45722908.0 -> -28745940.0  |  -27512076.0\n",
      "9 : reward: -52202328.0 -> -44053404.0  |  -43257928.0\n",
      "10 : reward: -41042832.0 -> -50346228.0  |  -49939188.0\n",
      "11 : reward: -53710768.0 -> -57041148.0  |  -56201312.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 27 --------------------------------\n",
      "0 : reward: -120627.1796875 -> 6340.81982421875  |  6350.888671875\n",
      "1 : reward: -9967554.0 -> 6061.857421875  |  6052.23291015625\n",
      "2 : reward: -67406.15625 -> 5500.97802734375  |  6653.1611328125\n",
      "3 : reward: -10423382.0 -> -106434.828125  |  -159495.125\n",
      "4 : reward: -30839212.0 -> -226298.953125  |  -213097.53125\n",
      "5 : reward: -35697.3515625 -> -561862.3125  |  -568097.25\n",
      "6 : reward: -8806139.0 -> -1185284.875  |  -854217.125\n",
      "7 : reward: -2029941.25 -> -1283303.75  |  -3880064.75\n",
      "8 : reward: -19989600.0 -> -11732544.0  |  -11123485.0\n",
      "9 : reward: -21239414.0 -> -20071702.0  |  -18520236.0\n",
      "10 : reward: -15684306.0 -> -38835380.0  |  -39390776.0\n",
      "11 : reward: -57559772.0 -> -57853248.0  |  -57715376.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 28 --------------------------------\n",
      "0 : reward: 6139.93212890625 -> 6754.22509765625  |  6702.0947265625\n",
      "1 : reward: -3386408.5 -> -31914.67578125  |  -19585.466796875\n",
      "2 : reward: -36644.5390625 -> -229280.921875  |  -380037.1875\n",
      "3 : reward: -2765909.0 -> -1788227.625  |  -1815691.375\n",
      "4 : reward: -19843286.0 -> -3704278.75  |  -3995410.0\n",
      "5 : reward: -2406758.5 -> -6498815.5  |  -10802932.0\n",
      "6 : reward: -7505123.0 -> -13717483.0  |  -15968420.0\n",
      "7 : reward: -4025798.25 -> -17929294.0  |  -17747196.0\n",
      "8 : reward: -39628536.0 -> -38068168.0  |  -41462232.0\n",
      "9 : reward: -56179748.0 -> -47585932.0  |  -47471672.0\n",
      "10 : reward: -48734436.0 -> -52555752.0  |  -53545964.0\n",
      "11 : reward: -55232572.0 -> -54656236.0  |  -54329128.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 29 --------------------------------\n",
      "0 : reward: -828110.9375 -> 6271.4814453125  |  6243.22607421875\n",
      "1 : reward: -203987.40625 -> -17280.974609375  |  5842.5556640625\n",
      "2 : reward: -189905.34375 -> -66481.640625  |  -63555.01953125\n",
      "3 : reward: -13648509.0 -> -224349.15625  |  -127092.484375\n",
      "4 : reward: -2728099.25 -> -2106912.5  |  -2170827.25\n",
      "5 : reward: -922860.6875 -> -3188798.75  |  -1941372.0\n",
      "6 : reward: -8599386.0 -> -4139842.5  |  -4484101.0\n",
      "7 : reward: -36640260.0 -> -18579430.0  |  -18309812.0\n",
      "8 : reward: -17384144.0 -> -29797408.0  |  -27113568.0\n",
      "9 : reward: -9275012.0 -> -32868538.0  |  -33159332.0\n",
      "10 : reward: -41877136.0 -> -39590172.0  |  -39600796.0\n",
      "11 : reward: -44651980.0 -> -53438908.0  |  -53897944.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 30 --------------------------------\n",
      "0 : reward: -43078.4453125 -> 6737.84521484375  |  7302.86474609375\n",
      "1 : reward: -434880.46875 -> 6054.33642578125  |  6044.70947265625\n",
      "2 : reward: -1345.1015625 -> 5276.33544921875  |  5269.15234375\n",
      "3 : reward: -147820.78125 -> -331088.25  |  -311802.3125\n",
      "4 : reward: -939462.625 -> -10285958.0  |  -10486630.0\n",
      "5 : reward: -265075.78125 -> -17030654.0  |  -16937064.0\n",
      "6 : reward: -771052.9375 -> -20047742.0  |  -19166076.0\n",
      "7 : reward: -24152456.0 -> -27738372.0  |  -31653452.0\n",
      "8 : reward: -31445676.0 -> -29578208.0  |  -30009288.0\n",
      "9 : reward: -33809148.0 -> -32071974.0  |  -31701594.0\n",
      "10 : reward: -44431544.0 -> -38019284.0  |  -37904792.0\n",
      "11 : reward: -45994632.0 -> -41958804.0  |  -49829144.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 31 --------------------------------\n",
      "0 : reward: -43859540.0 -> 6342.8828125  |  6268.3408203125\n",
      "1 : reward: -24505342.0 -> 1959.069580078125  |  2135.54833984375\n",
      "2 : reward: -6874244.0 -> -33047.4921875  |  -32821.921875\n",
      "3 : reward: -26975.3046875 -> -179204.296875  |  -155702.84375\n",
      "4 : reward: -16292146.0 -> -847183.625  |  -887132.5\n",
      "5 : reward: -1039575.875 -> -1032566.9375  |  -2124935.0\n",
      "6 : reward: 5177.689453125 -> -2477188.75  |  -2218975.0\n",
      "7 : reward: -54134104.0 -> -15032074.0  |  -14846028.0\n",
      "8 : reward: -13894790.0 -> -22611454.0  |  -22783084.0\n",
      "9 : reward: -32775168.0 -> -32156502.0  |  -35381000.0\n",
      "10 : reward: -51750080.0 -> -46871396.0  |  -46883576.0\n",
      "11 : reward: -49738384.0 -> -50468012.0  |  -50148656.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 32 --------------------------------\n",
      "0 : reward: -143813.5 -> 6172.24462890625  |  6166.142578125\n",
      "1 : reward: -15502.607421875 -> 3865.93212890625  |  -7299.01708984375\n",
      "2 : reward: -369751.8125 -> -33722.74609375  |  -109781.9375\n",
      "3 : reward: -24706722.0 -> -2299472.5  |  -2388274.0\n",
      "4 : reward: -11983.595703125 -> -2971776.5  |  -5107.1767578125\n",
      "5 : reward: -4974462.0 -> -4975762.5  |  -4954188.0\n",
      "6 : reward: -416936.0 -> -23234116.0  |  -22996160.0\n",
      "7 : reward: -10043608.0 -> -34101012.0  |  -33776348.0\n",
      "8 : reward: -30395896.0 -> -34414368.0  |  -32423160.0\n",
      "9 : reward: -15973446.0 -> -44679724.0  |  -44496900.0\n",
      "10 : reward: -55265748.0 -> -55988040.0  |  -55230224.0\n",
      "11 : reward: -58889984.0 -> -59214100.0  |  -58922556.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 33 --------------------------------\n",
      "0 : reward: 4931.6064453125 -> 1080.1763916015625  |  1097.80908203125\n",
      "1 : reward: -152991.8125 -> -128062.7890625  |  4926.44677734375\n",
      "2 : reward: -2848565.5 -> -239057.453125  |  -216035.0\n",
      "3 : reward: -29277528.0 -> -256543.109375  |  -44260.79296875\n",
      "4 : reward: -3657718.0 -> -332958.8125  |  -261973.21875\n",
      "5 : reward: -183262.90625 -> -1269597.5  |  -641649.25\n",
      "6 : reward: 4237.0185546875 -> -4719572.0  |  -4699993.0\n",
      "7 : reward: -18371692.0 -> -5899541.0  |  -5801884.0\n",
      "8 : reward: -9967343.0 -> -8047711.5  |  -7662345.5\n",
      "9 : reward: -24325940.0 -> -41370612.0  |  -41618240.0\n",
      "10 : reward: -54285624.0 -> -48478588.0  |  -49048580.0\n",
      "11 : reward: -56741032.0 -> -57212192.0  |  -56903512.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n",
      "True\n",
      " Iteration 34 --------------------------------\n",
      "0 : reward: -557966.6875 -> 4546.7255859375  |  3275.31640625\n",
      "1 : reward: -10804.91015625 -> -3047.262939453125  |  -2762.9267578125\n",
      "2 : reward: 6255.158203125 -> -30027.724609375  |  -219136.6875\n",
      "3 : reward: -26216400.0 -> -200521.171875  |  3148.64697265625\n",
      "4 : reward: -55312776.0 -> -825324.1875  |  -3442122.0\n",
      "5 : reward: -49180616.0 -> -11779498.0  |  -11308972.0\n",
      "6 : reward: -31357272.0 -> -22049278.0  |  -21642932.0\n",
      "7 : reward: -15528398.0 -> -23626736.0  |  -27957784.0\n",
      "8 : reward: -1629424.25 -> -24468236.0  |  -24246656.0\n",
      "9 : reward: -11581793.0 -> -25871742.0  |  -25547124.0\n",
      "10 : reward: -28881806.0 -> -28123552.0  |  -29427800.0\n",
      "11 : reward: -10143790.0 -> -36364460.0  |  -36371120.0\n",
      "Best reward so far:  7853.363\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "noise_std = starting_noise_std\n",
    "for i in range(50):\n",
    "    noise_keys = jax.random.split(noise_keys[0], num=jax.local_device_count())\n",
    "    train_keys = jax.random.split(noise_keys[0], num=jax.local_device_count())\n",
    "\n",
    "    policy_params_with_noise, noise = add_noise_pmap(policy_params, noise_std, noise_keys)\n",
    "    #policy_params_with_noise, noise1,noise2 = add_pareto_noise_pmap(policy_params, noise_beta, noise_scale, noise_keys)\n",
    "    \n",
    "    rewards_before, obs, acts, states_before = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "    policy_params_trained, rewards_lists = do_apg_pmap(apg_epochs, env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, learning_rate, episode_length, action_repeat, normalize_observations, batch_size, clipping, truncation_length)\n",
    "    rewards_after, obs, acts, states_after = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params_trained, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "            \n",
    "    print(jnp.any(policy_params_trained['params']['Dense_0']['kernel'] - policy_params_with_noise['params']['Dense_0']['kernel']))\n",
    "    \n",
    "    top_idx = sorted(range(len(rewards_lists)), key=lambda k: jnp.mean(rewards_lists[k][-5:]), reverse=True)\n",
    "        \n",
    "    _, params_def = jax.tree_flatten(policy_params)\n",
    "    params_flat, _ = jax.tree_flatten(policy_params_trained)\n",
    "    top_params_flat = [param[top_idx[0]] for param in params_flat]\n",
    "    top_params = jax.tree_unflatten(params_def, top_params_flat)\n",
    "    \n",
    "#     _, norm_def = jax.tree_flatten(normalizer_params)\n",
    "#     norm_flat, _ = jax.tree_flatten(normalizer_params_all)\n",
    "#     top_norm_flat = [param[top_idx[0]] for param in norm_flat]\n",
    "#     top_norms = jax.tree_unflatten(norm_def, top_norm_flat)\n",
    "    \n",
    "    noise_beta -= .1\n",
    "    noise_std  += .01\n",
    "    \n",
    "    if jnp.mean(rewards_lists[top_idx[0]][-5:]) > best_reward:\n",
    "        noise_beta = 2.0\n",
    "        noise_std =starting_noise_std\n",
    "        policy_params = top_params\n",
    "        top_normalizer_params = normalizer_params\n",
    "        best_reward = jnp.mean(rewards_lists[top_idx[0]][-5:])\n",
    "        \n",
    "    normalizer_params = obs_normalizer_update_fn(normalizer_params, obs[top_idx[0],:])\n",
    "    meta_rewards_list.append(best_reward)\n",
    "    \n",
    "    print(f\" Iteration {i} --------------------------------\")\n",
    "    \n",
    "    for j in range(len(top_idx)):\n",
    "        done_idx = jnp.where(states_before.done[top_idx[j], :], size=1)[0].item()\n",
    "        if done_idx == 0:\n",
    "            done_idx = rewards_before.shape[-1]\n",
    "        rewards_sum_before = jnp.sum(rewards_before[top_idx[j],:done_idx])\n",
    "\n",
    "        done_idx = jnp.where(states_after.done[top_idx[j], :], size=1)[0].item()\n",
    "        if done_idx == 0:\n",
    "            done_idx = rewards_after.shape[-1]\n",
    "        rewards_sum_after = jnp.sum(rewards_after[top_idx[j],:done_idx])\n",
    "        \n",
    "        print(f\"{j} : reward: {rewards_sum_before} -> {jnp.mean(rewards_lists[top_idx[j]][-5:])}  |  {rewards_sum_after}\")\n",
    "        \n",
    "        \n",
    "    if i %10 == 0: \n",
    "        plt.plot(rewards_lists[top_idx[0]])\n",
    "        plt.figure()\n",
    "    \n",
    "    #print(f\"{i} : best reward: {rewards_sum_before} -> {rewards_lists[top_idx[0]][-1]}  |  {rewards_sum_after}\")\n",
    "\n",
    "    print(\"Best reward so far: \", best_reward)\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(meta_rewards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brax.jumpy as jp\n",
    "@jax.jit\n",
    "def do_rnn_rollout(policy_params, normalizer_params, key):\n",
    "    init_state = env.reset(key)\n",
    "    h0 = jp.zeros_like(init_state.obs)\n",
    "\n",
    "    def do_one_rnn_step(carry, step_idx):\n",
    "        state, h, policy_params, normalizer_params  = carry\n",
    "\n",
    "        normed_obs = obs_normalizer_apply_fn(normalizer_params, state.obs)\n",
    "        h1 , actions = policy.apply(policy_params, h, normed_obs)\n",
    "        #actions = jp.ones_like(actions)*0.0\n",
    "        nstate = env.step(state, actions)    \n",
    "        #h1 = jax.lax.cond(nstate.done, lambda x: jnp.zeros_like(h1), lambda x: h1, None)\n",
    "        return (jax.lax.stop_gradient(nstate), h1, policy_params, normalizer_params), (nstate.reward,state.obs, actions, nstate)\n",
    "\n",
    "\n",
    "    _, (rewards, obs, acts, states) = jp.scan(\n",
    "        do_one_rnn_step, (init_state, h0, policy_params, normalizer_params),\n",
    "        (jnp.array(range(episode_length // action_repeat))),\n",
    "        length=episode_length // action_repeat)\n",
    "\n",
    "    return rewards, obs, acts, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, reset_key = jax.random.split(key)\n",
    "(rewards, obs, acts, states) = do_rnn_rollout(policy_params, top_normalizer_params, reset_key)\n",
    "\n",
    "done_idx = jnp.where(states.done, size=1)[0].item()\n",
    "if done_idx == 0:\n",
    "    done_idx = states.done.shape[0]\n",
    "rewards_sum = jnp.sum(rewards[:done_idx])\n",
    "\n",
    "plt.plot(obs);\n",
    "plt.figure()\n",
    "plt.plot(acts);\n",
    "print(rewards_sum)\n",
    "print(states.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_flat, qp_def = jax.tree_flatten(states.qp)\n",
    "\n",
    "qp_list = []\n",
    "\n",
    "for i in range(qp_flat[0].shape[0]):\n",
    "    qpc=[]\n",
    "    for thing in qp_flat:\n",
    "        qpc.append(thing[i,:])\n",
    "    qp_list.append(jax.tree_unflatten(qp_def, qpc))\n",
    "    \n",
    "\n",
    "visualize(env.sys, qp_list, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(obs**2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brax",
   "language": "python",
   "name": "brax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
