{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=12'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "from brax.training import normalization\n",
    "\n",
    "\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.envs import create_fn\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from controllers import GruController, MlpController\n",
    "from common import do_local_apg, add_guassian_noise, add_uniform_noise, add_uniform_and_pareto_noise, add_sym_pareto_noise, do_one_rollout\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "def visualize(sys, qps, height=480):\n",
    "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
    "  return HTML(html.render(sys, qps, height=height))\n",
    "\n",
    "len(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length = 500\n",
    "action_repeat = 1\n",
    "batch_size = jax.local_device_count()\n",
    "#noise_std = 0.2\n",
    "\n",
    "noise_scale = 5.0\n",
    "noise_beta = 1.8\n",
    "\n",
    "\n",
    "apg_epochs = 50\n",
    "batch_size = 16\n",
    "truncation_length = 50\n",
    "learning_rate = 1e-4\n",
    "clipping = 1e9\n",
    "\n",
    "normalize_observations=True\n",
    "\n",
    "env_name = \"reacher\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=None, auto_reset=False)\n",
    "env = env_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "reset_keys = jax.random.split(key, num=jax.local_device_count())\n",
    "_, model_key = jax.random.split(reset_keys[0])\n",
    "noise_keys = jax.random.split(model_key, num=jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GruController(env.observation_size,env.action_size,64)\n",
    "normalizer_params, obs_normalizer_update_fn, obs_normalizer_apply_fn = normalization.create_observation_normalizer(\n",
    "          env.observation_size, normalize_observations, num_leading_batch_dims=1)\n",
    "\n",
    "add_noise_pmap = jax.pmap(add_uniform_noise, in_axes=(None,None,0))\n",
    "add_pareto_noise_pmap = jax.pmap(add_uniform_and_pareto_noise, in_axes=(None,None,None,0))\n",
    "\n",
    "do_apg_pmap = jax.pmap(do_local_apg, in_axes = (None,None,None,None,0,0,None,None,None,None,None,None), static_broadcasted_argnums=(0,1,2,6,7,8,9,10,11,12))\n",
    "do_rollout_pmap = jax.pmap(do_one_rollout, in_axes = (None,None,None,0,0,None,None,None), static_broadcasted_argnums=(0,1,5,6,7))\n",
    "\n",
    "\n",
    "init_states = jax.pmap(env.reset)(reset_keys)\n",
    "x0 = init_states.obs\n",
    "h0 = jnp.zeros(env.observation_size)\n",
    "\n",
    "policy_params = policy.init(model_key, h0, x0)\n",
    "\n",
    "best_reward = -float('inf')\n",
    "meta_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " Iteration 0 --------------------------------\n",
      "0 : reward: -130.35806274414062 -> -94.97229766845703  |  -82.41363525390625\n",
      "1 : reward: -122.38020324707031 -> -98.6775131225586  |  -90.70228576660156\n",
      "2 : reward: -128.6324462890625 -> -99.26054382324219  |  -96.81623840332031\n",
      "3 : reward: -115.21902465820312 -> -100.17466735839844  |  -86.3016357421875\n",
      "4 : reward: -90.1900863647461 -> -101.42979431152344  |  -79.38787078857422\n",
      "5 : reward: -150.99293518066406 -> -105.2579116821289  |  -121.22537231445312\n",
      "6 : reward: -112.30643463134766 -> -105.89470672607422  |  -92.74269104003906\n",
      "7 : reward: -199.127685546875 -> -110.51299285888672  |  -122.61164093017578\n",
      "8 : reward: -106.36737060546875 -> -111.6080322265625  |  -95.34896087646484\n",
      "9 : reward: -102.63705444335938 -> -112.7664794921875  |  -100.23857879638672\n",
      "10 : reward: -157.7981414794922 -> -123.7837142944336  |  -129.56370544433594\n",
      "11 : reward: -308.7178955078125 -> -146.25743103027344  |  -127.78128814697266\n",
      "Best reward so far:  -87.40067\n",
      "--------------------------------------\n",
      "CPU times: user 2min 16s, sys: 1.45 s, total: 2min 17s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1):\n",
    "    noise_keys = jax.random.split(noise_keys[0], num=jax.local_device_count())\n",
    "    train_keys = jax.random.split(noise_keys[0], num=jax.local_device_count())\n",
    "\n",
    "    #policy_params_with_noise, noise = add_noise_pmap(policy_params, noise_std, noise_keys)\n",
    "    policy_params_with_noise, noise1,noise2 = add_pareto_noise_pmap(policy_params, noise_beta, noise_scale, noise_keys)\n",
    "    \n",
    "    rewards_before, obs, acts, states_before = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "    policy_params_trained, rewards_lists = do_apg_pmap(apg_epochs, env_fn, policy.apply, normalizer_params, policy_params_with_noise, train_keys, learning_rate, episode_length, action_repeat, normalize_observations, batch_size, clipping, truncation_length)\n",
    "    rewards_after, obs, acts, states_after = do_rollout_pmap(env_fn, policy.apply, normalizer_params, policy_params_trained, train_keys, episode_length, action_repeat, normalize_observations)\n",
    "            \n",
    "    print(jnp.any(policy_params_trained['params']['Dense_1']['kernel'] - policy_params_with_noise['params']['Dense_1']['kernel']))\n",
    "    \n",
    "    top_idx = sorted(range(len(rewards_lists)), key=lambda k: jnp.mean(rewards_lists[k][-5:]), reverse=True)\n",
    "    \n",
    "    normalizer_params = obs_normalizer_update_fn(normalizer_params, obs[top_idx[0],:])\n",
    "    \n",
    "    _, params_def = jax.tree_flatten(policy_params)\n",
    "    params_flat, _ = jax.tree_flatten(policy_params_trained)\n",
    "    top_params_flat = [param[top_idx[0]] for param in params_flat]\n",
    "    top_params = jax.tree_unflatten(params_def, top_params_flat)\n",
    "    \n",
    "    \n",
    "#     _, norm_def = jax.tree_flatten(normalizer_params)\n",
    "#     norm_flat, _ = jax.tree_flatten(normalizer_params_all)\n",
    "#     top_norm_flat = [param[top_idx[0]] for param in norm_flat]\n",
    "#     top_norms = jax.tree_unflatten(norm_def, top_norm_flat)\n",
    "    \n",
    "    noise_beta -= .1\n",
    "    \n",
    "    if rewards_lists[top_idx[0]][-1] > best_reward:\n",
    "        noise_beta = 2.0\n",
    "        policy_params = top_params\n",
    "        top_normalizer_params = normalizer_params\n",
    "        best_reward = jnp.mean(rewards_lists[top_idx[0]][-5:])\n",
    "        \n",
    "    meta_rewards_list.append(best_reward)\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\" Iteration {i} --------------------------------\")\n",
    "    \n",
    "    for j in range(len(top_idx)):\n",
    "        done_idx = jnp.where(states_before.done[top_idx[j], :], size=1)[0].item()\n",
    "        if done_idx == 0:\n",
    "            done_idx = rewards_before.shape[-1]\n",
    "        rewards_sum_before = jnp.sum(rewards_before[top_idx[j],:done_idx])\n",
    "\n",
    "        done_idx = jnp.where(states_after.done[top_idx[j], :], size=1)[0].item()\n",
    "        if done_idx == 0:\n",
    "            done_idx = rewards_after.shape[-1]\n",
    "        rewards_sum_after = jnp.sum(rewards_after[top_idx[j],:done_idx])\n",
    "        \n",
    "        print(f\"{j} : reward: {rewards_sum_before} -> {jnp.mean(rewards_lists[top_idx[j]][-5:])}  |  {rewards_sum_after}\")\n",
    "\n",
    "    \n",
    "    #print(f\"{i} : best reward: {rewards_sum_before} -> {rewards_lists[top_idx[0]][-1]}  |  {rewards_sum_after}\")\n",
    "\n",
    "    print(\"Best reward so far: \", best_reward)\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_params_trained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17002/3834341962.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_params_trained\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dense_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpolicy_params_with_noise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dense_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_params_trained' is not defined"
     ]
    }
   ],
   "source": [
    "jnp.any(policy_params_trained['params']['Dense_1']['kernel'] - policy_params_with_noise['params']['Dense_1']['kernel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brax.jumpy as jp\n",
    "@jax.jit\n",
    "def do_rnn_rollout(policy_params, normalizer_params, key):\n",
    "    init_state = env.reset(key)\n",
    "    h0 = jp.zeros_like(init_state.obs)\n",
    "\n",
    "    def do_one_rnn_step(carry, step_idx):\n",
    "        state, h, policy_params, normalizer_params  = carry\n",
    "\n",
    "        normed_obs = obs_normalizer_apply_fn(normalizer_params, state.obs)\n",
    "        h1 , actions = policy.apply(policy_params, h, normed_obs)\n",
    "        #actions = jp.ones_like(actions)*0.0\n",
    "        nstate = env.step(state, actions)    \n",
    "        #h1 = jax.lax.cond(nstate.done, lambda x: jnp.zeros_like(h1), lambda x: h1, None)\n",
    "        return (jax.lax.stop_gradient(nstate), h1, policy_params, normalizer_params), (nstate.reward,state.obs, actions, nstate)\n",
    "\n",
    "\n",
    "    _, (rewards, obs, acts, states) = jp.scan(\n",
    "        do_one_rnn_step, (init_state, h0, policy_params, normalizer_params),\n",
    "        (jnp.array(range(episode_length // action_repeat))),\n",
    "        length=episode_length // action_repeat)\n",
    "\n",
    "    return rewards, obs, acts, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, reset_key = jax.random.split(key)\n",
    "(rewards, obs, acts, states) = do_rnn_rollout(policy_params, top_normalizer_params, reset_key)\n",
    "\n",
    "done_idx = jnp.where(states.done, size=1)[0].item()\n",
    "rewards_sum = jnp.sum(rewards[:done_idx])\n",
    "\n",
    "plt.plot(obs[:done_idx,:]);\n",
    "plt.figure()\n",
    "plt.plot(acts);\n",
    "print(rewards_sum)\n",
    "print(states.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, obs, acts, states = do_one_rollout(env_fn, policy.apply, top_normalizer_params, top_params, key, episode_length, action_repeat, normalize_observations)\n",
    "print(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_flat, qp_def = jax.tree_flatten(states.qp)\n",
    "\n",
    "qp_list = []\n",
    "\n",
    "for i in range(qp_flat[0].shape[0]):\n",
    "    qpc=[]\n",
    "    for thing in qp_flat:\n",
    "        qpc.append(thing[i,:])\n",
    "    qp_list.append(jax.tree_unflatten(qp_def, qpc))\n",
    "    \n",
    "\n",
    "visualize(env.sys, qp_list, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brax",
   "language": "python",
   "name": "brax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
