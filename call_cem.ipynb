{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import pickle\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=24'\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "#from jax.config import config; config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "from brax.training import normalization\n",
    "\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "from brax.envs import create_fn\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from controllers import GruController, MlpController, LinearController\n",
    "\n",
    "from ce_apg import do_one_rollout, cem_apg\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "def visualize(sys, qps, height=480):\n",
    "  \"\"\"Renders a 3D visualization of the environment.\"\"\"\n",
    "  return HTML(html.render(sys, qps, height=height))\n",
    "\n",
    "len(jax.devices())\n",
    "\n",
    "save_dir = \"save_reacher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "env_name = \"reacher\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "\n",
    "episode_length = 500\n",
    "action_repeat = 1\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=None, auto_reset=False)\n",
    "env = env_fn()\n",
    "\n",
    "policy_size = int(2**jnp.ceil(jnp.log2(env.observation_size*4)))\n",
    "print(policy_size)\n",
    "policy = GruController(env.observation_size, env.action_size, 128)\n",
    "\n",
    "for i in range(1):\n",
    "    inference_fn, params, rewards = cem_apg(env_fn,\n",
    "                                            20,\n",
    "                                            key=jax.random.PRNGKey(i),\n",
    "                                            episode_length = episode_length,\n",
    "                                            action_repeat = action_repeat,\n",
    "                                            apg_epochs = 30,\n",
    "                                            cem_epochs = 1,\n",
    "                                            batch_size = 1,\n",
    "                                            truncation_length = None,\n",
    "                                            learning_rate = 5e-4,\n",
    "                                            clipping = 1e9,\n",
    "                                            initial_std = 0.01,\n",
    "                                            num_elite = 8,\n",
    "                                            eps = 0.0,\n",
    "                                            normalize_observations=True,\n",
    "                                            policy = policy\n",
    "                                           )\n",
    "\n",
    "\n",
    "\n",
    "    pickle.dump(params, open(f\"{save_dir}/{env_name}_policy{i}.pkl\", 'wb'))\n",
    "    #pickle.dump(no, open(f\"{save_dir}/{env_name}_normalize{i}.pkl\", 'wb'))\n",
    "    pickle.dump(rewards, open(f\"{save_dir}/{env_name}_rewards.pkl{i}\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brax.model\n",
    "\n",
    "model.save_params('/tmp/params', params)\n",
    "inference_fn = ce_apg.make_inference_fn(\n",
    "    env.observation_size, env.action_size, True, policy)\n",
    "params = model.load_params('/tmp/params')\n",
    "\n",
    "jit_env_reset = jax.jit(env.reset)\n",
    "jit_env_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "rollout = []\n",
    "rng = jax.random.PRNGKey(seed=0)\n",
    "state = jit_env_reset(rng=rng)\n",
    "while not state.done:\n",
    "  rollout.append(state)\n",
    "  act_rng, rng = jax.random.split(rng)\n",
    "  act = jit_inference_fn(params, state.obs, act_rng)\n",
    "  state = jit_env_step(state, act)\n",
    "\n",
    "HTML(html.render(env.sys, [s.qp for s in rollout]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"acrobot\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "\n",
    "episode_length = 500\n",
    "action_repeat = 1\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=None, auto_reset=False)\n",
    "env = env_fn()\n",
    "\n",
    "\n",
    "\n",
    "policy_size = int(2**jnp.ceil(jnp.log2(env.observation_size*4)))\n",
    "print(policy_size)\n",
    "policy = GruController(env.observation_size, env.action_size, 32)\n",
    "\n",
    "for i in range(8):\n",
    "    normalizer_params, policy_params, rewards = cem_apg(env_fn,\n",
    "                                                        200,\n",
    "                                                        key=jax.random.PRNGKey(i),\n",
    "                                                        episode_length = episode_length,\n",
    "                                                        action_repeat = action_repeat,\n",
    "                                                        apg_epochs = 75,\n",
    "                                                        cem_epochs = 1,\n",
    "                                                        batch_size = 1,\n",
    "                                                        truncation_length = None,\n",
    "                                                        learning_rate = 5e-4,\n",
    "                                                        clipping = 1e9,\n",
    "                                                        initial_std = 0.01,\n",
    "                                                        num_elite = 8,\n",
    "                                                        eps = 0.0,\n",
    "                                                        normalize_observations=True,\n",
    "                                                        policy = policy\n",
    "                                                       )\n",
    "\n",
    "\n",
    "\n",
    "    pickle.dump(policy_params, open(f\"{save_dir}/{env_name}_policy{i}.pkl\", 'wb'))\n",
    "    pickle.dump(normalizer_params, open(f\"{save_dir}/{env_name}_normalize{i}.pkl\", 'wb'))\n",
    "    pickle.dump(rewards, open(f\"{save_dir}/{env_name}_rewards.pkl{i}\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"inverted_double_pendulum_swingup\"  # @param ['ant', 'humanoid', 'fetch', 'grasp', 'halfcheetah', 'walker2d, 'ur5e', 'reacher', bball_1dof]\n",
    "\n",
    "episode_length = 500\n",
    "action_repeat = 1\n",
    "env_fn = create_fn(env_name = env_name, action_repeat=action_repeat, batch_size=None, auto_reset=False)\n",
    "env = env_fn()\n",
    "\n",
    "\n",
    "\n",
    "policy_size = int(2**jnp.ceil(jnp.log2(env.observation_size*4)))\n",
    "print(policy_size)\n",
    "policy = GruController(env.observation_size, env.action_size, 128)\n",
    "\n",
    "for i in range(8):\n",
    "    normalizer_params, policy_params, rewards = cem_apg(env_fn,\n",
    "                                                        200,\n",
    "                                                        key=jax.random.PRNGKey(i),\n",
    "                                                        episode_length = episode_length,\n",
    "                                                        action_repeat = action_repeat,\n",
    "                                                        apg_epochs = 75,\n",
    "                                                        cem_epochs = 1,\n",
    "                                                        batch_size = 1,\n",
    "                                                        truncation_length = None,\n",
    "                                                        learning_rate = 5e-4,\n",
    "                                                        clipping = 1e9,\n",
    "                                                        initial_std = 0.01,\n",
    "                                                        num_elite = 8,\n",
    "                                                        eps = 0.0,\n",
    "                                                        normalize_observations=True,\n",
    "                                                        policy = policy\n",
    "                                                       )\n",
    "\n",
    "\n",
    "\n",
    "    pickle.dump(policy_params, open(f\"{save_dir}/{env_name}_policy{i}.pkl\", 'wb'))\n",
    "    pickle.dump(normalizer_params, open(f\"{save_dir}/{env_name}_normalize{i}.pkl\", 'wb'))\n",
    "    pickle.dump(rewards, open(f\"{save_dir}/{env_name}_rewards.pkl{i}\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(policy_params2, open(\"inverted_double_pendulum_swingup.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"inverted_double_pendulum_swingup\"\n",
    "pickle.dump(policy_params2, open(f\"{env_name}_policy.pkl\", 'wb'))\n",
    "pickle.dump(normalizer_params2, open(f\"{env_name}_normalize.pkl\", 'wb'))\n",
    "pickle.dump(rewards2, open(f\"{env_name}_rewards.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brax",
   "language": "python",
   "name": "brax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
